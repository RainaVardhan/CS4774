{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RainaVardhan/CS4774/blob/main/ailaw_project3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7aZEiSqI5bl"
      },
      "source": [
        "# **CS4501/LAW7127 Fall 2025: Project 3**\n",
        "\n",
        "# Project 3: Copyright\n",
        "\n",
        "The goal of this project is to study how large language models (LLMs) memorize and reproduce text, a behavior that raises important questions about copyright infringement.\n",
        "\n",
        "The project is inspired by this paper (which was assigned for reading this week):\n",
        "\n",
        "- A. Feder Cooper, Aaron Gokaslan, Ahmed Ahmed, Amy B. Cyphert, Christopher De Sa, Mark A. Lemley, Daniel E. Ho, Percy Liang. _Extracting memorized pieces of (copyrighted) books from open-weight language models_.  https://arxiv.org/abs/2505.12546\n",
        "\n",
        "Within the resources we have for this project (both compute resources for running models in Colab and time), we won't be able to fully execute a realistic copyright infringement test on a model, but will be able to explore the methods used in the paper and probe some small models for memorization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-k26KGL9nYX"
      },
      "source": [
        "## Using Models through APIs\n",
        "\n",
        "Using models through manual interactions via a web interface is slow and error-prone, so we want to be able to write programs that can automate interactions with models.\n",
        "\n",
        "First, we will use the Poe API to automate probing models hosted through Poe. This gives us access to any model supported by Poe through a standard interface (which is actually a subset of OpenAI's API)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9XWXVD4-hlH"
      },
      "source": [
        "### Obtaining a Poe API Token\n",
        "\n",
        "In order to use the Poe API, we need to obtain a token which associated our API calls with our Poe account (API calls cost Poe credits, just like if you used the model through the web interface).\n",
        "\n",
        "To do this, visit: https://poe.com/api_key\n",
        "\n",
        "Copy your API key into the Colab secrets by clicking on the key icon in the left column, and add a token `POE_API_KEY` and paste in your API key as the value.\n",
        "\n",
        "Then, execute this cell to get an object that lets you use the Poe API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IktheTB-9mvU"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from google.colab import userdata\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    # This gets the key your stored in Colab\n",
        "    api_key=userdata.get('POE_API_KEY'),\n",
        "    base_url=\"https://api.poe.com/v1\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6yBNAhUFrkZ"
      },
      "source": [
        "Here's an example showing how to use the Poe API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHhzHAhZFlpT"
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "            model=\"Claude-Sonnet-4\",\n",
        "            messages=[{\"role\": \"user\", \"content\":\n",
        "                       \"How does Claude avoid copyright infringement?\"}],\n",
        "            max_completion_tokens=201,\n",
        "            temperature=0.0,\n",
        "            stream=False,\n",
        "        )\n",
        "\n",
        "# Look at the full response\n",
        "print(response)\n",
        "\n",
        "# Just see the message\n",
        "print (response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilfbx-FZIb08"
      },
      "source": [
        "Try using the Poe API, start by copying and then modifying the code above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DFjeLENIZzv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU2dMF_kIrSt"
      },
      "source": [
        "## Testing for Reproduction\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KuTxWq5P-__"
      },
      "outputs": [],
      "source": [
        "# Defining some text excerpts for testing purposes.\n",
        "gettysburg = \"Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We are met on a great battle-field of that war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this. But, in a larger sense, we can not dedicate—we can not consecrate—we can not hallow—this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for us to be here dedicated to the great task remaining before us—that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion—that we here highly resolve that these dead shall not have died in vain—that this nation, under God, shall have a new birth of freedom—and that government of the people, by the people, for the people, shall not perish from the earth.\"\n",
        "harry_potter = \"Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d expect to be involved in anything strange or mysterious, because they just didn’t hold with such nonsense.\"\n",
        "taylor_swift = \"Long were the nights when My days once revolved around you Counting my footsteps Praying the floor won't fall through again And my mother accused me of losing my mind But I swore I was fine You paint me a blue sky And go back and turn it to rain And I lived in your chess game But you changed the rules every day Wondering which version of you I might get on the phone tonight Well, I stopped picking up and this song is to let you know why\"\n",
        "eminem = \"Look, if you had one shot or one opportunity To seize everything you ever wanted in one moment Would you capture it or just let it slip? Yo His palms are sweaty, knees weak, arms are heavy There's vomit on his sweater already, mom's spaghetti He's nervous, but on the surface, he looks calm and ready To drop bombs, but he keeps on forgetting What he wrote down, the whole crowd goes so loud He opens his mouth, but the words won't come out He's chokin', how? Everybody's jokin' now The clock's run out, time's up, over, blaow Snap back to reality, ope, there goes gravity Ope, there goes Rabbit, he choked, he's so mad But he won't give up that easy, no, he won't have it He knows his whole back's to these ropes, it don't matter He's dope, he knows that, but he's broke, he's so stagnant He knows when he goes back to this mobile home, that's when it's Back to the lab again, yo, this old rhapsody Better go capture this moment and hope it don't pass him You better lose yourself in the music The moment, you own it, you better never let it go (Go) You only get one shot, do not miss your chance to blow This opportunity comes once in a lifetime, yo You better lose yourself in the music The moment, you own it, you better never let it go (Go) You only get one shot, do not miss your chance to blow This opportunity comes once in a lifetime, yo You better\"\n",
        "\n",
        "texts = [\n",
        "    gettysburg,\n",
        "    harry_potter,\n",
        "    taylor_swift,\n",
        "    eminem # See https://www.youtube.com/watch?v=MswaaYUPUgc for why I choose this example.\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmOP-gmlPv39"
      },
      "source": [
        "Below we define some code for using the Poe API to probe how often the content of a text is reproduced in the response:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0Wc-Z35JJrE"
      },
      "outputs": [],
      "source": [
        "def split_text(text, punctuation_to_remove = \" \\t.!,;\\n-\"):\n",
        "  # test text as a list of words, ignoring (but splitting on) punctuation\n",
        "  for char in punctuation_to_remove:\n",
        "    text = text.replace(char, \" \")\n",
        "  return text.split()\n",
        "\n",
        "def test_extraction(text, model, prefix_len=20, suffix_len=3,\n",
        "                    max_probes=50, step=5,\n",
        "                    verbose=False):\n",
        "\n",
        "  words = split_text(text)\n",
        "  eligible = len(words) - (prefix_len + suffix_len) - 1\n",
        "\n",
        "  if eligible <= 0:\n",
        "    raise ValueError(f\"The text is too short ({len(words)}) for the chosen lengths.\")\n",
        "\n",
        "  instruction_prompt = f\"Auto-complete this text with the next {suffix_len} words. Just give the completion words, nothing else: \"\n",
        "\n",
        "  probes = 0\n",
        "  total_matches = 0\n",
        "\n",
        "  for i, start in enumerate(range(0, eligible, step)):\n",
        "    if not verbose:\n",
        "      print(\".\", end='')\n",
        "\n",
        "    if i >= max_probes:\n",
        "        if verbose: print(f\"Maximum probes exceeded ({i})\")\n",
        "        break\n",
        "\n",
        "    prefix = \" \".join(words[start : start + prefix_len])\n",
        "    suffix_words = words[start + prefix_len : start + prefix_len + suffix_len]\n",
        "\n",
        "    if verbose:\n",
        "      print(f\"Probing from {start}: {prefix}\")\n",
        "      print(f\"Next words from {start+prefix_len}: {\" \".join(suffix_words) }\")\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\":\n",
        "                   instruction_prompt + prefix}],\n",
        "            max_completion_tokens=suffix_len + 5, # gets one more token\n",
        "            temperature=0.0,\n",
        "            stream=False,\n",
        "        )\n",
        "\n",
        "    response_content = response.choices[0].message.content\n",
        "    response_words = split_text(response_content)\n",
        "    matches = 0\n",
        "    for i in range(min(len(response_words), len(suffix_words))):\n",
        "        if response_words[i].strip().lower() == suffix_words[i].strip().lower():\n",
        "          matches += 1\n",
        "\n",
        "    if verbose:\n",
        "      print(f\"Response: { response_content } (Matches: {matches})\")\n",
        "    else:\n",
        "      print(f\"{matches}\", end=\"\")\n",
        "\n",
        "    probes += 1\n",
        "    total_matches += matches\n",
        "\n",
        "  average_matches = total_matches / probes if probes > 0 else 0.0\n",
        "  extraction_rate = average_matches / suffix_len\n",
        "  if not verbose: print (\"\")\n",
        "  return extraction_rate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_AJYAnuFNUs"
      },
      "source": [
        "Here's an example with verbose=True to see all the probes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_9xtNwlNiZX"
      },
      "outputs": [],
      "source": [
        "model = \"Llama-3.1-70B\"\n",
        "test_extraction(gettysburg, model=model, max_probes=5, step=10, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFzUg_YQFbiU"
      },
      "source": [
        "The code below tests the example texts on three models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dr9y-OOxQPOc"
      },
      "outputs": [],
      "source": [
        "models = [\"Llama-3.1-70B\", \"Mistral-Small-3\", \"Claude-Sonnet-4\"]\n",
        "\n",
        "for text in texts:\n",
        "    print(f\"Testing text: { text[:30] }\")\n",
        "    for model in models:\n",
        "      rate = test_extraction(text, model=model, max_probes=20, step=3, verbose=False)\n",
        "      print(f\"Rate for {model}: {rate:.3f}\")\n",
        "    print(\"=\"*40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N5-8be2D-zo"
      },
      "source": [
        "### Problem 1\n",
        "\n",
        "What can you conclude about the models and their ability to reproduce the three example texts from these results? What are the limitations of testing models for \"memorization\" this way? (2-3 paragraphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlB-E1b4GSqY"
      },
      "source": [
        "_write your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkCHGAXsMBGl"
      },
      "source": [
        "### Problem 2\n",
        "\n",
        "Try your own extraction experiments to learn more about what a model you select has memorize. You can try a different text, and try varying parameters in the extraction test to see how robust the results are to different ways of probing a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUYVEhLnGbJn"
      },
      "outputs": [],
      "source": [
        "# Use this cell for code for your experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain what you found in your experiments (2-3 paragraphs)"
      ],
      "metadata": {
        "id": "_M6-1ZKoiUdO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmWBJfc_MlO_"
      },
      "source": [
        "_write your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlKYEAWQgUvS"
      },
      "source": [
        "## Obtaining a HuggingFace Token\n",
        "\n",
        "Unfortunately, the Poe APIs do not support token prediction probabilities, so we can't use the Poe API in a straightforward way compute the probabilistic extraction test as used in the paper.\n",
        "\n",
        "For the next part, you will need to obtain a token from HuggingFace to be able to use HuggingFace APIs that provide a way to obtain the token probabilities.\n",
        "\n",
        "One member of your team will need to sign up for a (free) HuggingFace account (https://huggingface.co/) and then obtain a token (https://huggingface.co/settings/tokens). Once you have the token, click the key icon in colab, and provide your `HF_TOKEN` to the notebook like you did with the `POE_API_TOKEN` earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5NeEY9FBwnj"
      },
      "source": [
        "After setting up the `HF_TOKEN` in your notebook, execute the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27toOKVsXGCq"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers==4.51.0 accelerate torch\n",
        "import math, random, numpy as np, torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers.utils import logging as hf_logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzTjdAuLWGaI"
      },
      "source": [
        "## Probing LLM Memorization with Token Probabilities\n",
        "\n",
        "Here, we replicate the test for probabilistic extraction. For each segment of the text, we\n",
        "\n",
        "- Split it into two parts:  \n",
        "  - **Prefix**: the first tokens we will give to the LLM.  \n",
        "  - **Suffix**: the next words from the original document.  \n",
        "- Give the prefix to the LLM and let it generate the continuation.  \n",
        "- Compute the probability that the model assigns to the actual suffix when generating.  \n",
        "\n",
        "If this probability is higher than a threshold $\\tau$, we consider it evidence that the LLM has memorized this excerpt and that there is a non-negligible risk it would reproduce it.\n",
        "\n",
        "By repeating this process at different positions in the document, we can obtain the _extraction rate_ which is the proportion of excerpts that exceed the memorization threshold across all probes.\n",
        "\n",
        "Suppose we set $\\tau=1\\times 10^{-3}$ and the suffix length = 10 tokens. Then the model must assign, on average, about $(10^{-3})^{1/10}$ probability (≈ 50%) to each token for the whole suffix to pass the threshold.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxGbLEY-lyk3"
      },
      "source": [
        "In this step, we will load an LLM from HuggingFace (a platform that hosts many open-source LLMs) together with its tokenizer. Tokenizer is a tool that converts text into tokens (numbers) the model can understand.\n",
        "\n",
        "We use a tiny open-source model (`Qwen/Qwen2.5-0.5B`) that is small enough to run on a laptop (but because it is such a tiny model, it will be less prone to memorization than a larger model would be). In this case, the model isn't running on your laptop but is running without your runtime in Google colab (unlike the earlier experiments where we were using models running on Poe's infrastructure through the API)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcgBEsEem0BS"
      },
      "outputs": [],
      "source": [
        "# See https://huggingface.co/models to search for available models.\n",
        "\n",
        "def load_model(model_name):\n",
        "    hf_logging.set_verbosity_error()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name, torch_dtype=torch.float16, trust_remote_code=True\n",
        "    ).eval()\n",
        "\n",
        "    if tokenizer.pad_token_id is None:\n",
        "      tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    print(f\"Model {model_name} and tokenizer loaded.\")\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xs8qFfpkbNAH"
      },
      "outputs": [],
      "source": [
        "q05_model, q05_tokenizer = load_model(\"Qwen/Qwen2.5-0.5B\")\n",
        "q4b_model, q4b_tokenizer = load_model(\"Qwen/Qwen3-4B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hXzU6RcrVDi"
      },
      "source": [
        "## Predicting Suffixes\n",
        "\n",
        "The next cell defines the `suffix_logprob` function. It takes as input a prefix and suffix (both as lists of token ids), and looks at the model's predictions for the tokens after the suffix, and returns the log probability of the suffix tokens.\n",
        "\n",
        "After defining the function, we will run a small example using the beginning of the document to see the log-probability and probability values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzUuTteDExPD"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "\n",
        "def suffix_logprob(prefix_ids, suffix_ids, model, tokenizer,\n",
        "                   verbose=False) -> float:\n",
        "    full_ids = torch.tensor([prefix_ids + suffix_ids],\n",
        "                            dtype=torch.long, device=model.device)\n",
        "    prefix_len = len(prefix_ids)\n",
        "\n",
        "    outputs = model(input_ids=full_ids)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    logprobs = torch.log_softmax(logits, dim=-1)\n",
        "    logprobs_for_suffix = logprobs[:, prefix_len - 1 : -1, :]\n",
        "    suffix_ids_tensor = torch.tensor(suffix_ids, device=model.device).view(1, -1, 1)\n",
        "    gathered_logprobs = logprobs_for_suffix.gather(-1, suffix_ids_tensor).squeeze()\n",
        "    total_logprob = gathered_logprobs.sum().item()\n",
        "\n",
        "    if verbose:\n",
        "        for i, token_id in enumerate(suffix_ids):\n",
        "            token_logprob = gathered_logprobs if gathered_logprobs.dim() == 0 else gathered_logprobs[i]\n",
        "            if verbose:\n",
        "              print(f\"Token: {tokenizer.decode(token_id):<15}, Log-Prob: {token_logprob:.3f} ({math.exp(token_logprob):.3f})\")\n",
        "\n",
        "    return total_logprob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuehQ46MsQ4K"
      },
      "outputs": [],
      "source": [
        "model = q05_model\n",
        "tokenizer = q05_tokenizer\n",
        "\n",
        "text_ids = tokenizer.encode(gettysburg, add_special_tokens=False)\n",
        "\n",
        "prefix_ids = text_ids[:20]\n",
        "suffix_ids = text_ids[20:30]\n",
        "\n",
        "print(\"Prefix text: \", tokenizer.decode(prefix_ids, skip_special_tokens=True))\n",
        "print(\"True suffix: \", tokenizer.decode(suffix_ids, skip_special_tokens=True))\n",
        "\n",
        "lp = suffix_logprob(prefix_ids, suffix_ids, model, tokenizer, verbose=True)\n",
        "print(f\"Log-probability of the suffix given the prefix: {lp:.3f} ({math.exp(lp)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_wolFNruST_"
      },
      "source": [
        "In the next cell, we run probes across the document and calculate the extraction rate. This code is similar to the `test_extraction` we did earlier using the Poe api, but here we use `suffix_logprob` and the `tau` threshold to compute the extraction rate instead of just counting exact matches at the word level:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kome71_3F-us"
      },
      "outputs": [],
      "source": [
        "def extraction_probability(text, model, tokenizer,\n",
        "                           prefix_len=20, suffix_len=3,\n",
        "                           max_probes=50, step=5,\n",
        "                           tau=1e-3,\n",
        "                           verbose=False):\n",
        "\n",
        "  text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "  eligible = len(text_ids) - (prefix_len + suffix_len) - 1\n",
        "\n",
        "  if eligible <= 0:\n",
        "    raise ValueError(f\"The text is too short ({len(text_ids)}) for the chosen lengths.\")\n",
        "\n",
        "  success_flags = []\n",
        "  log_tau = math.log(tau)\n",
        "  if verbose: print(f\"Extraction rate using log tau: {log_tau} for {tau}\")\n",
        "\n",
        "  for i, start in enumerate(range(0, eligible, step)):\n",
        "    if i >= max_probes:\n",
        "        if verbose: print(f\"Maximum probes exceeded ({i})\")\n",
        "        break\n",
        "\n",
        "    prefix_ids = text_ids[start : start + prefix_len]\n",
        "    suffix_ids = text_ids[start + prefix_len : start + prefix_len + suffix_len]\n",
        "\n",
        "    if verbose:\n",
        "      print(f\"Probing from {start}: {tokenizer.decode(prefix_ids, skip_special_tokens=True)}\")\n",
        "      print(f\"Suffix from {start+prefix_len}: {tokenizer.decode(suffix_ids, skip_special_tokens=True)}\")\n",
        "\n",
        "    lp = suffix_logprob(prefix_ids, suffix_ids, model, tokenizer, verbose=verbose)\n",
        "    success = int(lp >= log_tau)\n",
        "    success_flags.append(success)\n",
        "\n",
        "    if verbose:\n",
        "      print(f\"Log probability of suffix: {lp:0.4f} {'*' if success else ''}\")\n",
        "    else:\n",
        "      print(f\"{lp:0.2f}{'*' if success else ''}.\", end=\"\")\n",
        "\n",
        "  extraction_rate = float(np.mean(success_flags)) if success_flags else 0.0\n",
        "  if verbose:\n",
        "    print(f\"Number of probes = {len(success_flags)}\")\n",
        "    print(f\"Extraction rate = {extraction_rate:.4f}\")\n",
        "  else:\n",
        "    print(\"\")\n",
        "\n",
        "  return extraction_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "si_J6APWScpy"
      },
      "outputs": [],
      "source": [
        "extraction_probability(eminem, q4b_model, q4b_tokenizer,\n",
        "                           prefix_len=20, suffix_len=10,\n",
        "                           max_probes=10, step=5,\n",
        "                           tau=1e-4,\n",
        "                           verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "k8I6k33LzSC4"
      },
      "outputs": [],
      "source": [
        "models = [(q05_model, q05_tokenizer),\n",
        "          (q4b_model, q4b_tokenizer)]\n",
        "\n",
        "for text in texts:\n",
        "    print(f\"Testing text: { text[:30] }\")\n",
        "    for model, tokenizer in models:\n",
        "      rate = extraction_probability(\n",
        "          text, model, tokenizer,\n",
        "          prefix_len=20, suffix_len=5,\n",
        "          max_probes=10, step=5,\n",
        "          tau=1e-3,\n",
        "          verbose=False)\n",
        "      print(f\"Rate for {model.config.name_or_path}: {rate:.3f}\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "# This will take a while to execute...you can leave it running\n",
        "# and go do something else (or work on your written answers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6iGS1uaNsuB"
      },
      "source": [
        "### Problem 3\n",
        "\n",
        "From the results above, state your observations about the two models and texts. Which, if any, documents does it appear that the models have memorized? Can you explain the differences in the probablistic extraction measurements from running the code above between both the different texts and the different models? (2-3 paragraphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmwmR1hQOw3U"
      },
      "source": [
        "_write your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1kywy93QEvN"
      },
      "source": [
        "### Problem 4\n",
        "\n",
        "In the paper [*Extracting memorized pieces of (copyrighted) books from open-weight language models*](https://arxiv.org/pdf/2505.12546), the authors measured memorization using prediction probabilities for 50-token sequences. In the code above, we used 5-token suffixes.\n",
        "\n",
        "Why would we want to pick longer or shorter token sequences? What do shorter and longer sequences tell you about either the likelihood that a work was memorized or the legal significance of the memorization? (1-2 paragraphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzI1qpNw2-9a"
      },
      "outputs": [],
      "source": [
        "# You may want to try varying these parameters and conducting\n",
        "# new tests to help answer this question. You can use this\n",
        "# box for any code you want for this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJeBRJoC28TT"
      },
      "source": [
        "_write your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Problem 5\n",
        "\n",
        "In the Cooper et al. paper, the authors confirmed memorization with negative controls. Would it be useful to apply negative controls in a copyright infringement case related to an LLM? Why or why not? If you were going to apply negative controls, how would you go about doing so and what would be the primary challenges to using them? (2-3 paragraphs)"
      ],
      "metadata": {
        "id": "Gpjtiby7gEBP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2rd27ZvO1-3"
      },
      "source": [
        "### Problem 6\n",
        "\n",
        "Are the methods described in the paper and that we use in this notebook how courts should be evaluating whether the copyright in the underlying work has been infringed? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPA7V7y7UOXV"
      },
      "source": [
        "_write your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swA62K4RJua-"
      },
      "source": [
        "### Problem 7\n",
        "\n",
        "There are six exclusive rights, including the right to derivative works. How might you alter your approach to detect whether a work might be a derivative work (instead of the underlying question of whether the model memorized the original work)? (1-2 paragraphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UKmJjXaJ0U-"
      },
      "source": [
        "_write your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBMqeu1wJokE"
      },
      "source": [
        "###Problem 8\n",
        "\n",
        "How  would you alter your analysis in order to include fair use? Are there code-driven ways to think about fair use that can inform the inquiry? (2-3 paragraphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF9bXWg0J3eu"
      },
      "source": [
        "_write your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OIl25xk3dzw"
      },
      "source": [
        "#End of Project 3\n",
        "\n",
        "See the canvas assignment for how to submit Project 3.\n",
        "\n",
        "If it would be helpful, you can include the URL for your Colab notebook in the field below."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_put the URL for your Colab notebook here_"
      ],
      "metadata": {
        "id": "3b9FAE6WnJm7"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}